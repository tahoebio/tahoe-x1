integrations:
- integration_type: git_repo
  git_repo: tahoebio/tahoe-x1
  git_branch: main
  # git_commit:  # OR use your commit hash
  pip_install: -e . --no-deps
  ssh_clone: true  # Should be true if using a private repo

# We are fetching, converting, and training on the 'val' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `llm-foundry/scripts/train/provisioning-request.yaml`
# to convert and host the full 'train' dataset.
command: |
  cd tahoe-x1/scripts
  composer train.py /mnt/config/parameters.yaml
image: ghcr.io/tahoebio/tahoe-x1:1.0.0
env_variables:
  MOSAICML_PLATFORM: False #Logging metadata to MosaicML platform is disabled, since it seems to timeout
name: tx-70m

compute:
  gpus: 16 # Number of GPUs to use
  cluster: r15z1p1
scheduling:
  max_duration: 12 # Maximum duration of the run in hours

  ## These configurations are optional
  # cluster:
  # gpu_type: a100_80gb # Type of GPU to use. We use a100_80gb in our experiments


# The below is injected as a YAML file: /mnt/config/parameters.yaml
# but is not used in this example.
parameters:
  global_seed: 777
  seed: ${global_seed}
  device_train_batch_size: 400
  global_train_batch_size: 4800
  device_eval_batch_size: 400
  device_train_microbatch_size: "auto"
  vocabulary:
    remote: "s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json"
    local: "vocab.json"
  model:
    name: tahoe_x1
    d_model: 512
    n_layers: 12
    init_device: cpu
    expansion_ratio: 4
    standard_scale_outputs: False
    transformer_activation: relu
    n_heads: 8
    norm_scheme: "pre"
    use_generative_training: True
    use_cell_conditioned_generation: False
    use_glu: False
    cell_emb_style: cls
    attn_config:
      attn_impl: triton
      attn_type: "grouped_query_attention"
      kv_nheads: 8
      attn_pdrop: 0.0
    norm_config:
      norm_type: "layernorm"
      eps: 1.0e-5
    expression_encoder:
      input_emb_style: "continuous"
      dropout: 0.1
      max_value: 512
      activation: relu
      use_norm: True
    gene_encoder:
      use_norm: True
    mvc:
      arch_style: "inner product"
      query_activation: "sigmoid"
      scaled_dot_product: True
    expression_decoder:
      n_outputs: 1
      n_layers: 1
      activation: "leaky_relu"
  collator:
    do_padding: True
    pad_value: -2
    do_mlm: True
    do_binning: True
    mlm_probability: 0.5
    mask_value: -1
    max_length: 1024
    sampling: True
    data_style: "both"
    num_bins: 51
    right_binning: False
    use_junk_tokens: False
  train_loader:
    dataset:
      streams:
        tahoe:
          remote: "s3://tahoe-hackathon-data/MFM/tahoe_100m_MDS_v1/train/"
          local: "mds-data-folder/tahoe/train"
        cellxgene:
          remote: "s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/"
          local: "mds-data-folder/cellxgene/train"
      download_timeout: 300
      allow_unsafe_types: True
      shuffle: True
      shuffle_seed: ${global_seed}
      num_canonical_nodes: 2
    drop_last: False
    num_workers: 8
    pin_memory: True
    prefetch_factor: 48
    persistent_workers: True
  valid_loader:
    dataset:
      streams:
        tahoe:
          remote: "s3://tahoe-hackathon-data/MFM/tahoe_100m_MDS_v1/valid/"
          local: "mds-data-folder/tahoe/val"
        cellxgene:
          remote: "s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/"
          local: "mds-data-folder/cellxgene/val"
      download_timeout: 300
      allow_unsafe_types: True
      shuffle: False
      shuffle_seed: ${global_seed}
      num_canonical_nodes: 2
    drop_last: False
    num_workers: 8
    pin_memory: True
    prefetch_factor: 48
    persistent_workers: True
  optimizer:
    name: decoupled_adamw
    lr: 3.0e-4
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 1.0e-05
  scheduler:
    name: cosine_with_warmup
    t_warmup: "0.05dur"
    t_max: "1dur"
    alpha_f: 0.1
  algorithms:
    gradient_clipping:
      clipping_type: norm
      clipping_threshold: 1.0
    low_precision_layernorm: {}
  precision: amp_bf16
  eval_interval: "1000ba"
  max_duration: "6ep"
  fsdp_config:
    sharding_strategy: FULL_SHARD
    mixed_precision: DEFAULT
    activation_checkpointing: false
    activation_checkpointing_reentrant: false
    activation_cpu_offload: false
    limit_all_gathers: true
    verbose: true
  callbacks:
    speed_monitor:
      window_size: 20
    lr_monitor: {}
    memory_monitor: {}
    runtime_estimator: {}
  loggers:
    wandb:
      project: tahoe_x1
      log_artifacts: False
  save_folder: "s3://tahoe-hackathon-data/MFM/ckpts/{run_name}"
  save_interval: "2000ba"
  autoresume: True
