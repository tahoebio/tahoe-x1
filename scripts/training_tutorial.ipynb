{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahoe-x1 Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to train a Tahoe-x1 model from scratch or fine-tune a pre-trained model.\n",
    "\n",
    "### 0. Prerequisites\n",
    "- Access to GPU resources (NVIDIA H100/H200 recommended)\n",
    "- Tahoe-x1 package installed (Refer to README for the installation guide)\n",
    "- Access to training data (see README for dataset information) \n",
    "    - You either need to have the training data locally in your machine or provide the aws s3 credentials so that the data can be strimmed from our public s3 bucket (recommended)\n",
    "- Weights & Biases account (optional, for logging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Customize Config\n",
    "\n",
    "You can start with the `test_run.yaml` which is a sample config on how training the 70M model  and customize it for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoex\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0003\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-05\n",
      "scheduler:\n",
      "  name: cosine_with_warmup\n",
      "  t_warmup: 0.05dur\n",
      "  t_max: 1dur\n",
      "  alpha_f: 0.1\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 6ep\n",
      "fsdp_config:\n",
      "  sharding_strategy: FULL_SHARD\n",
      "  mixed_precision: DEFAULT\n",
      "  activation_checkpointing: false\n",
      "  activation_checkpointing_reentrant: false\n",
      "  activation_cpu_offload: false\n",
      "  limit_all_gathers: true\n",
      "  verbose: true\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoex\n",
      "    log_artifacts: false\n",
      "save_folder: s3://tahoe-hackathon-data/MFM/ckpts/{run_name}\n",
      "save_interval: 2000ba\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Load the base configuration\n",
    "cfg = om.load(\"../configs/test_run.yaml\")\n",
    "print(om.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to: ./my_training_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Customize the config based on your system, design choice, etc\n",
    "\n",
    "# Training settings\n",
    "cfg.global_train_batch_size = 256  # Total batch size across all devices\n",
    "cfg.max_duration = \"20ba\" #\"2ep\"  # Train for 2 epochs (adjust as needed)\n",
    "\n",
    "# Model configuration\n",
    "cfg.model.d_model = 512\n",
    "cfg.model.n_layers = 12\n",
    "cfg.model.n_heads = 8\n",
    "\n",
    "# IMPORTANT: Current codebase only supports flash attention without attention mask\n",
    "cfg.model.attn_config.attn_impl = \"flash\"\n",
    "cfg.model.attn_config.use_attn_mask = False\n",
    "\n",
    "# Data loader settings\n",
    "cfg.train_loader.num_workers = 4  # Adjust based on your system\n",
    "cfg.train_loader.prefetch_factor = 2 # Adjust based on your system\n",
    "\n",
    "cfg.collator.use_chem_token=False # You can set it to True if your training data includes drug info(such as Tahoe100M) and you want to inject that to the model\n",
    "# Optimizer settings\n",
    "cfg.optimizer.lr = 3.0e-4\n",
    "cfg.optimizer.weight_decay = 1.0e-05\n",
    "\n",
    "# Logging\n",
    "cfg.run_name = \"custom_test_run\"\n",
    "cfg.loggers.wandb.project = \"tahoex-tutorial\"\n",
    "save_folder = cfg.save_folder = f\"./checkpoints/{cfg.run_name}\"\n",
    "cfg.save_interval = \"500ba\"  # Save every 500 batches\n",
    "\n",
    "# Save the config\n",
    "custom_config_path = \"./my_training_config.yaml\"\n",
    "om.save(cfg, custom_config_path)\n",
    "print(f\"Configuration saved to: {custom_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training from Scratch\n",
    "\n",
    "#### Option A: Train using the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/llmfoundry/callbacks/env_logging_callback.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/tahoe/tahoe-x1/scripts/train.py:337: UserWarning: FSDP is not applicable for single-GPU training. Reverting to DDP.\n",
      "  warnings.warn(\n",
      "2025-10-21 20:51:36,435: rank0[325359][MainThread]: INFO: train: Downloading vocab...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading the file from s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json: Unable to locate credentials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1021 20:51:36.856222259 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "2025-10-21 20:51:36,984: rank0[325359][MainThread]: INFO: train: Setting vocab size to: 62720\n",
      "2025-10-21 20:51:37,175: rank0[325359][MainThread]: INFO: train: Building DataLoaders...\n",
      "2025-10-21 20:51:39,636: rank0[325359][MainThread]: INFO: train: train set number of samples: 60746795\n",
      "2025-10-21 20:51:39,679: rank0[325359][MainThread]: INFO: train: Validation set number of samples: 613604\n",
      "2025-10-21 20:51:40,025: rank0[325359][MainThread]: INFO: tahoex.model.model: MosaicML recommends using config.init_device=\"meta\" with Composer + FSDP for faster initialization.\n",
      "2025-10-21 20:51:41,142: rank0[325359][MainThread]: INFO: train: Total parameters: 70.996993 M\n",
      "2025-10-21 20:51:41,143: rank0[325359][MainThread]: INFO: train: Total trainable parameters: 70.996993 M \n",
      "2025-10-21 20:51:41,144: rank0[325359][MainThread]: INFO: train: gene_encoder: 32.113664 M parameters\n",
      "2025-10-21 20:51:41,144: rank0[325359][MainThread]: INFO: train: flag_encoder: 0.001024 M parameters\n",
      "2025-10-21 20:51:41,144: rank0[325359][MainThread]: INFO: train: expression_encoder: 0.264704 M parameters\n",
      "2025-10-21 20:51:41,145: rank0[325359][MainThread]: INFO: train: transformer_encoder: 37.829632 M parameters\n",
      "2025-10-21 20:51:41,146: rank0[325359][MainThread]: INFO: train: expression_decoder: 0.263169 M parameters\n",
      "2025-10-21 20:51:41,146: rank0[325359][MainThread]: INFO: train: mvc_decoder: 0.5248 M parameters\n",
      "2025-10-21 20:51:41,148: rank0[325359][MainThread]: INFO: train: Building Trainer...\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/trainer/trainer.py:247: UserWarning: `device_train_microbatch_size='auto'` may potentially fail with unexpected CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory errors and adjust the batch size, but it is possible CUDA will be put into an irrecoverable state due to PyTorch bugs, e.g. integer overflow. In this case, please manually set device_train_microbatch_size explicitly to an integer instead.\n",
      "  warnings.warn((\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfarjvd\u001b[0m (\u001b[33mvevotx\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tahoe/tahoe-x1/scripts/wandb/run-20251021_205142-yuv8a8l0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vevotx/tahoex-tutorial/runs/yuv8a8l0' target=\"_blank\">custom_test_run</a></strong> to <a href='https://wandb.ai/vevotx/tahoex-tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vevotx/tahoex-tutorial' target=\"_blank\">https://wandb.ai/vevotx/tahoex-tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vevotx/tahoex-tutorial/runs/yuv8a8l0' target=\"_blank\">https://wandb.ai/vevotx/tahoex-tutorial/runs/yuv8a8l0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/utils/checkpoint.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(_ensure_valid_checkpoint(checkpoint_filepath), map_location=map_location)\n",
      "2025-10-21 20:51:44,866: rank0[325359][MainThread]: INFO: train: Logging config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 777\n",
      "device_train_batch_size: 256\n",
      "global_train_batch_size: 256\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoex\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "  use_chem_token: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 4\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 2\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0003\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-05\n",
      "scheduler:\n",
      "  name: cosine_with_warmup\n",
      "  t_warmup: 0.05dur\n",
      "  t_max: 1dur\n",
      "  alpha_f: 0.1\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 20ba\n",
      "fsdp_config: null\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoex-tutorial\n",
      "    log_artifacts: false\n",
      "save_folder: ./checkpoints/custom_test_run\n",
      "save_interval: 500ba\n",
      "run_name: custom_test_run\n",
      "n_gpus: 1\n",
      "device_train_grad_accum: auto\n",
      "merge: true\n",
      "vocab_size: 62720\n",
      "train_dataset_size: 60746795\n",
      "valid_dataset_size: 613604\n",
      "n_params: 70996993\n",
      "n_trainable_params: 70996993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 20:51:45,240: rank0[325359][MainThread]: INFO: train: Starting training...\n",
      "******************************\n",
      "Config:\n",
      "composer_commit_hash: None\n",
      "composer_version: 0.28.0\n",
      "enabled_algorithms/GradientClipping: true\n",
      "enabled_algorithms/LowPrecisionLayerNorm: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 777\n",
      "time/remaining_estimate_unit: hours\n",
      "\n",
      "******************************\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/trainer/trainer.py:368: RuntimeWarning: CUDA out of memory or excessive memory allocation retries detected. Train microbatch size will be decreased from 256 -> 128.\n",
      "  warnings.warn(\n",
      "[batch=11/20]:\n",
      "\t Train time/batch: 10\n",
      "\t Train time/sample: 2560\n",
      "\t Train time/batch_in_epoch: 10\n",
      "\t Train time/sample_in_epoch: 2560\n",
      "\t Train memory/current_allocated_mem: 1.4400\n",
      "\t Train memory/current_active_mem: 1.4400\n",
      "\t Train memory/current_inactive_mem: 5.4282\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.0906\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 342.6001\n",
      "\t Train metrics/train/MSE: 342.9906\n",
      "\t Train metrics/train/MVC: 342.2750\n",
      "\t Train time/train: 0.0088\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0088\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "[batch=12/20]:\n",
      "\t Train time/batch: 11\n",
      "\t Train time/sample: 2816\n",
      "\t Train time/batch_in_epoch: 11\n",
      "\t Train time/sample_in_epoch: 2816\n",
      "\t Train memory/current_allocated_mem: 1.4337\n",
      "\t Train memory/current_active_mem: 1.4337\n",
      "\t Train memory/current_inactive_mem: 3.5282\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.0906\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 342.2568\n",
      "\t Train metrics/train/MSE: 342.5580\n",
      "\t Train metrics/train/MVC: 341.9615\n",
      "\t Train time/train: 0.0090\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0090\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "\t Train time/remaining_estimate: 0.0018\n",
      "[batch=13/20]:\n",
      "\t Train time/batch: 12\n",
      "\t Train time/sample: 3072\n",
      "\t Train time/batch_in_epoch: 12\n",
      "\t Train time/sample_in_epoch: 3072\n",
      "\t Train memory/current_allocated_mem: 1.4330\n",
      "\t Train memory/current_active_mem: 1.4330\n",
      "\t Train memory/current_inactive_mem: 5.0304\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.0906\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 344.4078\n",
      "\t Train metrics/train/MSE: 345.5250\n",
      "\t Train metrics/train/MVC: 343.2681\n",
      "\t Train time/train: 0.0092\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0092\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0016\n",
      "[batch=14/20]:\n",
      "\t Train time/batch: 13\n",
      "\t Train time/sample: 3328\n",
      "\t Train time/batch_in_epoch: 13\n",
      "\t Train time/sample_in_epoch: 3328\n",
      "\t Train memory/current_allocated_mem: 1.4333\n",
      "\t Train memory/current_active_mem: 1.4333\n",
      "\t Train memory/current_inactive_mem: 5.5670\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.2262\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 342.7674\n",
      "\t Train metrics/train/MSE: 344.7888\n",
      "\t Train metrics/train/MVC: 340.7953\n",
      "\t Train time/train: 0.0094\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0094\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0013\n",
      "[batch=15/20]:\n",
      "\t Train time/batch: 14\n",
      "\t Train time/sample: 3584\n",
      "\t Train time/batch_in_epoch: 14\n",
      "\t Train time/sample_in_epoch: 3584\n",
      "\t Train memory/current_allocated_mem: 1.4343\n",
      "\t Train memory/current_active_mem: 1.4343\n",
      "\t Train memory/current_inactive_mem: 4.0623\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.2262\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 340.9297\n",
      "\t Train metrics/train/MSE: 342.2828\n",
      "\t Train metrics/train/MVC: 339.5650\n",
      "\t Train time/train: 0.0096\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0096\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0011\n",
      "[batch=16/20]:\n",
      "\t Train time/batch: 15\n",
      "\t Train time/sample: 3840\n",
      "\t Train time/batch_in_epoch: 15\n",
      "\t Train time/sample_in_epoch: 3840\n",
      "\t Train memory/current_allocated_mem: 1.4342\n",
      "\t Train memory/current_active_mem: 1.4342\n",
      "\t Train memory/current_inactive_mem: 3.1208\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.2262\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 340.8876\n",
      "\t Train metrics/train/MSE: 340.4188\n",
      "\t Train metrics/train/MVC: 341.3066\n",
      "\t Train time/train: 0.0099\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0099\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0009\n",
      "[batch=17/20]:\n",
      "\t Train time/batch: 16\n",
      "\t Train time/sample: 4096\n",
      "\t Train time/batch_in_epoch: 16\n",
      "\t Train time/sample_in_epoch: 4096\n",
      "\t Train memory/current_allocated_mem: 1.4332\n",
      "\t Train memory/current_active_mem: 1.4332\n",
      "\t Train memory/current_inactive_mem: 2.0544\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.2262\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 341.4311\n",
      "\t Train metrics/train/MSE: 341.4189\n",
      "\t Train metrics/train/MVC: 341.4292\n",
      "\t Train time/train: 0.0101\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0101\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0007\n",
      "[batch=18/20]:\n",
      "\t Train time/batch: 17\n",
      "\t Train time/sample: 4352\n",
      "\t Train time/batch_in_epoch: 17\n",
      "\t Train time/sample_in_epoch: 4352\n",
      "\t Train memory/current_allocated_mem: 1.4326\n",
      "\t Train memory/current_active_mem: 1.4326\n",
      "\t Train memory/current_inactive_mem: 5.7019\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 339.4441\n",
      "\t Train metrics/train/MSE: 340.2065\n",
      "\t Train metrics/train/MVC: 338.8644\n",
      "\t Train time/train: 0.0103\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0103\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0004\n",
      "[batch=19/20]:\n",
      "\t Train time/batch: 18\n",
      "\t Train time/sample: 4608\n",
      "\t Train time/batch_in_epoch: 18\n",
      "\t Train time/sample_in_epoch: 4608\n",
      "\t Train memory/current_allocated_mem: 1.4327\n",
      "\t Train memory/current_active_mem: 1.4327\n",
      "\t Train memory/current_inactive_mem: 5.0307\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 341.5416\n",
      "\t Train metrics/train/MSE: 342.1382\n",
      "\t Train metrics/train/MVC: 340.9495\n",
      "\t Train time/train: 0.0105\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0105\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0002\n",
      "[batch=20/20]:\n",
      "\t Train time/batch: 19\n",
      "\t Train time/sample: 4864\n",
      "\t Train time/batch_in_epoch: 19\n",
      "\t Train time/sample_in_epoch: 4864\n",
      "\t Train memory/current_allocated_mem: 1.4384\n",
      "\t Train memory/current_active_mem: 1.4384\n",
      "\t Train memory/current_inactive_mem: 2.4498\n",
      "\t Train memory/current_reserved_mem: 26.4090\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 339.4916\n",
      "\t Train metrics/train/MSE: 339.3168\n",
      "\t Train metrics/train/MVC: 339.7860\n",
      "\t Train time/train: 0.0108\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0108\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0000\n",
      "[Eval batch=1/100] Eval on eval data\n",
      "[Eval batch=11/100] Eval on eval data\n",
      "[Eval batch=21/100] Eval on eval data\n",
      "[Eval batch=31/100] Eval on eval data\n",
      "[Eval batch=41/100] Eval on eval data\n",
      "[Eval batch=50/100] Eval on eval data\n",
      "[Eval batch=60/100] Eval on eval data\n",
      "[Eval batch=70/100] Eval on eval data\n",
      "[Eval batch=80/100] Eval on eval data\n",
      "[Eval batch=90/100] Eval on eval data\n",
      "[Eval batch=100/100] Eval on eval data:\n",
      "\t Eval metrics/eval/MSE: 337.8651\n",
      "\t Eval metrics/eval/MVC: 339.8668\n",
      "\t Eval metrics/eval/Spearman: 0.2027\n",
      "2025-10-21 20:52:46,342: rank0[325359][MainThread]: INFO: train: Training finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and checkpoints saved at ./checkpoints/custom_test_run!\n"
     ]
    }
   ],
   "source": [
    "from train import main\n",
    "cfg = om.load(custom_config_path)\n",
    "\n",
    "# Train the model\n",
    "trainer = main(cfg)\n",
    "\n",
    "print(f\"Training completed and checkpoints saved at {save_folder}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Train using Composer CLI\n",
    "\n",
    "Alternatively, you can train using the command line with composer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tahoe/tahoe-x1/.venv/bin/python3: can't open file '/tahoe/tahoe-x1/scripts/../train.py': [Errno 2] No such file or directory\n",
      "ERROR:composer.cli.launcher:Rank 0 crashed with exit code 2.\n",
      "Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.\n",
      "Global rank 0 (PID 303486) exited with code 2\n",
      "ERROR:composer.cli.launcher:Global rank 0 (PID 303486) exited with code 2\n"
     ]
    }
   ],
   "source": [
    "# Run training via shell command\n",
    "!composer ../train.py -f {custom_config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Resume Training\n",
    "\n",
    "Note that if your run stopped unexpectedly and you want to resume the training from where it stopped, simply use the **same `run_name` and `save_folder`** in the configuration. The trainer will automatically pick up from the last saved checkpoint.\n",
    "The trainer will automatically detect existing checkpoints and resume with full state (model weights, optimizer, scheduler, etc.).\n",
    "\n",
    "```python\n",
    "resume_cfg = om.load(custom_config_path)\n",
    "\n",
    "# Keep the same run_name and save_folder - training will auto-resume\n",
    "trainer = main(resume_cfg)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tuning a Pre-trained Model\n",
    "\n",
    "When loading from a checkpoint, you have two options:\n",
    "\n",
    "**Option 1:** Full Recovery\n",
    "- Set `load_path` to your checkpoint directory or file\n",
    "- Loads both model weights AND optimizer/scheduler states\n",
    "\n",
    "```python\n",
    "cfg.load_path = \"s3://bucket/path/to/checkpoint/\"\n",
    "# This recovers everything: weights + optimizer + scheduler\n",
    "```\n",
    "\n",
    "**Option 2:** Weights Only \n",
    "- Set `load_path` AND `load_weights_only=True`\n",
    "- Loads **only model weights**, optimizer/scheduler are initialized fresh\n",
    "\n",
    "```python\n",
    "cfg.load_path = \"s3://bucket/path/to/checkpoint/\"\n",
    "cfg.load_weights_only = True\n",
    "# This loads only weights, optimizer/scheduler start fresh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning configuration:\n",
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoex\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 1.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-06\n",
      "scheduler:\n",
      "  name: constant_with_warmup\n",
      "  t_warmup: 0ba\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 30ba\n",
      "fsdp_config:\n",
      "  sharding_strategy: FULL_SHARD\n",
      "  mixed_precision: DEFAULT\n",
      "  activation_checkpointing: false\n",
      "  activation_checkpointing_reentrant: false\n",
      "  activation_cpu_offload: false\n",
      "  limit_all_gathers: true\n",
      "  verbose: true\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoex\n",
      "    log_artifacts: false\n",
      "save_folder: ./checkpoints/finetuned_{run_name}\n",
      "save_interval: 2000ba\n",
      "load_path: s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load configuration for fine-tuning\n",
    "finetune_cfg = om.load(\"../configs/test_run.yaml\")\n",
    "\n",
    "# Set checkpoint path\n",
    "checkpoint_path = \"s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\"  # Or local path\n",
    "finetune_cfg.load_path = checkpoint_path\n",
    "\n",
    "# Adjust learning rate for fine-tuning and schedular for finetuning\n",
    "finetune_cfg.optimizer.lr = 1.0e-5\n",
    "finetune_cfg.optimizer.weight_decay = 1.0e-6\n",
    "finetune_cfg.scheduler = {}\n",
    "finetune_cfg.scheduler.name = 'constant_with_warmup'\n",
    "finetune_cfg.scheduler.t_warmup = '0ba'\n",
    "\n",
    "# Shorter training duration for fine-tuning\n",
    "finetune_cfg.max_duration = \"30ba\"\n",
    "\n",
    "# Update save folder\n",
    "finetune_cfg.save_folder = \"./checkpoints/finetuned_{run_name}\"\n",
    "\n",
    "print(\"Fine-tuning configuration:\")\n",
    "print(om.to_yaml(finetune_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:37:14,410: rank0[325359][MainThread]: INFO: train: Downloading vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:37:17,234: rank0[325359][MainThread]: INFO: train: Setting vocab size to: 62720\n",
      "2025-10-21 21:37:17,244: rank0[325359][MainThread]: INFO: train: Building DataLoaders...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:37:25,160: rank0[325359][MainThread]: INFO: train: train set number of samples: 60746795\n",
      "2025-10-21 21:37:26,840: rank0[325359][MainThread]: INFO: train: Validation set number of samples: 613604\n",
      "2025-10-21 21:37:27,223: rank0[325359][MainThread]: INFO: tahoex.model.model: MosaicML recommends using config.init_device=\"meta\" with Composer + FSDP for faster initialization.\n",
      "2025-10-21 21:37:28,361: rank0[325359][MainThread]: INFO: train: Total parameters: 70.996993 M\n",
      "2025-10-21 21:37:28,362: rank0[325359][MainThread]: INFO: train: Total trainable parameters: 70.996993 M \n",
      "2025-10-21 21:37:28,363: rank0[325359][MainThread]: INFO: train: gene_encoder: 32.113664 M parameters\n",
      "2025-10-21 21:37:28,363: rank0[325359][MainThread]: INFO: train: flag_encoder: 0.001024 M parameters\n",
      "2025-10-21 21:37:28,364: rank0[325359][MainThread]: INFO: train: expression_encoder: 0.264704 M parameters\n",
      "2025-10-21 21:37:28,365: rank0[325359][MainThread]: INFO: train: transformer_encoder: 37.829632 M parameters\n",
      "2025-10-21 21:37:28,366: rank0[325359][MainThread]: INFO: train: expression_decoder: 0.263169 M parameters\n",
      "2025-10-21 21:37:28,366: rank0[325359][MainThread]: INFO: train: mvc_decoder: 0.5248 M parameters\n",
      "2025-10-21 21:37:28,368: rank0[325359][MainThread]: INFO: train: Building Trainer...\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/trainer/trainer.py:247: UserWarning: `device_train_microbatch_size='auto'` may potentially fail with unexpected CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory errors and adjust the batch size, but it is possible CUDA will be put into an irrecoverable state due to PyTorch bugs, e.g. integer overflow. In this case, please manually set device_train_microbatch_size explicitly to an integer instead.\n",
      "  warnings.warn((\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tahoe/tahoe-x1/scripts/wandb/run-20251021_213728-eu1p57e6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vevotx/tahoex/runs/eu1p57e6' target=\"_blank\">1761082648-flat-chimpanzee</a></strong> to <a href='https://wandb.ai/vevotx/tahoex' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vevotx/tahoex' target=\"_blank\">https://wandb.ai/vevotx/tahoex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vevotx/tahoex/runs/eu1p57e6' target=\"_blank\">https://wandb.ai/vevotx/tahoex/runs/eu1p57e6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/optim/scheduler.py:778: UserWarning: The warmup duration is 0. If you specified warmup as a fraction of total\n",
      "training duration, take note that the warmup duration is calculated in the\n",
      "same unit as the trainer's max_duration parameter.\n",
      "  warnings.warn(\n",
      "Downloading MFM/ckpts/70m/best-model.pt:   0%|          | 262k/284M [00:07<2:07:30, 37.1kiB/s]\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/utils/checkpoint.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(_ensure_valid_checkpoint(checkpoint_filepath), map_location=map_location)\n",
      "2025-10-21 21:37:40,472: rank0[325359][MainThread]: INFO: train: Logging config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoex\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 1.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-06\n",
      "scheduler:\n",
      "  name: constant_with_warmup\n",
      "  t_warmup: 0ba\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 30ba\n",
      "fsdp_config: null\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoex\n",
      "    log_artifacts: false\n",
      "save_folder: ./checkpoints/finetuned_1761082648-flat-chimpanzee\n",
      "save_interval: 2000ba\n",
      "load_path: s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\n",
      "n_gpus: 1\n",
      "device_train_grad_accum: auto\n",
      "merge: true\n",
      "run_name: 1761082648-flat-chimpanzee\n",
      "vocab_size: 62720\n",
      "train_dataset_size: 60746795\n",
      "valid_dataset_size: 613604\n",
      "n_params: 70996993\n",
      "n_trainable_params: 70996993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:37:41,201: rank0[325359][MainThread]: INFO: train: Starting training...\n",
      "******************************\n",
      "Config:\n",
      "composer_commit_hash: None\n",
      "composer_version: 0.28.0\n",
      "enabled_algorithms/GradientClipping: true\n",
      "enabled_algorithms/LowPrecisionLayerNorm: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 777\n",
      "time/remaining_estimate_unit: hours\n",
      "\n",
      "******************************\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/optim/scheduler.py:778: UserWarning: The warmup duration is 0. If you specified warmup as a fraction of total\n",
      "training duration, take note that the warmup duration is calculated in the\n",
      "same unit as the trainer's max_duration parameter.\n",
      "  warnings.warn(\n",
      "[batch=1/30]:\n",
      "\t Train time/epoch: 0\n",
      "\t Train time/batch: 0\n",
      "\t Train time/sample: 0\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train memory/current_allocated_mem: 1.8710\n",
      "\t Train memory/current_active_mem: 1.8710\n",
      "\t Train memory/current_inactive_mem: 4.2276\n",
      "\t Train memory/current_reserved_mem: 20.5190\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 196.7836\n",
      "\t Train metrics/train/MSE: 194.1176\n",
      "\t Train metrics/train/MVC: 199.4495\n",
      "\t Train time/train: 0.0632\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0632\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=2/30]:\n",
      "\t Train time/batch: 1\n",
      "\t Train time/sample: 100\n",
      "\t Train time/batch_in_epoch: 1\n",
      "\t Train time/sample_in_epoch: 100\n",
      "\t Train memory/current_allocated_mem: 2.4497\n",
      "\t Train memory/current_active_mem: 2.4497\n",
      "\t Train memory/current_inactive_mem: 4.9322\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 199.2352\n",
      "\t Train metrics/train/MSE: 196.7616\n",
      "\t Train metrics/train/MVC: 201.7088\n",
      "\t Train time/train: 0.0635\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0635\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0026\n",
      "[batch=3/30]:\n",
      "\t Train time/batch: 2\n",
      "\t Train time/sample: 200\n",
      "\t Train time/batch_in_epoch: 2\n",
      "\t Train time/sample_in_epoch: 200\n",
      "\t Train memory/current_allocated_mem: 2.4432\n",
      "\t Train memory/current_active_mem: 2.4432\n",
      "\t Train memory/current_inactive_mem: 3.2191\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 200.2868\n",
      "\t Train metrics/train/MSE: 197.6673\n",
      "\t Train metrics/train/MVC: 202.9063\n",
      "\t Train time/train: 0.0636\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0636\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0025\n",
      "[batch=4/30]:\n",
      "\t Train time/batch: 3\n",
      "\t Train time/sample: 300\n",
      "\t Train time/batch_in_epoch: 3\n",
      "\t Train time/sample_in_epoch: 300\n",
      "\t Train memory/current_allocated_mem: 2.4401\n",
      "\t Train memory/current_active_mem: 2.4401\n",
      "\t Train memory/current_inactive_mem: 3.2180\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 199.9130\n",
      "\t Train metrics/train/MSE: 197.5794\n",
      "\t Train metrics/train/MVC: 202.2466\n",
      "\t Train time/train: 0.0637\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0637\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0024\n",
      "[batch=5/30]:\n",
      "\t Train time/batch: 4\n",
      "\t Train time/sample: 400\n",
      "\t Train time/batch_in_epoch: 4\n",
      "\t Train time/sample_in_epoch: 400\n",
      "\t Train memory/current_allocated_mem: 2.4476\n",
      "\t Train memory/current_active_mem: 2.4476\n",
      "\t Train memory/current_inactive_mem: 5.0372\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 194.5947\n",
      "\t Train metrics/train/MSE: 192.0697\n",
      "\t Train metrics/train/MVC: 197.1196\n",
      "\t Train time/train: 0.0638\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0638\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0023\n",
      "[batch=6/30]:\n",
      "\t Train time/batch: 5\n",
      "\t Train time/sample: 500\n",
      "\t Train time/batch_in_epoch: 5\n",
      "\t Train time/sample_in_epoch: 500\n",
      "\t Train memory/current_allocated_mem: 2.4410\n",
      "\t Train memory/current_active_mem: 2.4410\n",
      "\t Train memory/current_inactive_mem: 3.5757\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.3611\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 193.3685\n",
      "\t Train metrics/train/MSE: 190.9273\n",
      "\t Train metrics/train/MVC: 195.8096\n",
      "\t Train time/train: 0.0639\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0639\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0022\n",
      "[batch=7/30]:\n",
      "\t Train time/batch: 6\n",
      "\t Train time/sample: 600\n",
      "\t Train time/batch_in_epoch: 6\n",
      "\t Train time/sample_in_epoch: 600\n",
      "\t Train memory/current_allocated_mem: 2.4415\n",
      "\t Train memory/current_active_mem: 2.4415\n",
      "\t Train memory/current_inactive_mem: 6.1862\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 196.2426\n",
      "\t Train metrics/train/MSE: 193.9935\n",
      "\t Train metrics/train/MVC: 198.4916\n",
      "\t Train time/train: 0.0640\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0640\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0021\n",
      "[batch=8/30]:\n",
      "\t Train time/batch: 7\n",
      "\t Train time/sample: 700\n",
      "\t Train time/batch_in_epoch: 7\n",
      "\t Train time/sample_in_epoch: 700\n",
      "\t Train memory/current_allocated_mem: 2.4396\n",
      "\t Train memory/current_active_mem: 2.4396\n",
      "\t Train memory/current_inactive_mem: 4.9424\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 195.8886\n",
      "\t Train metrics/train/MSE: 193.7275\n",
      "\t Train metrics/train/MVC: 198.0496\n",
      "\t Train time/train: 0.0665\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0665\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0096\n",
      "[batch=9/30]:\n",
      "\t Train time/batch: 8\n",
      "\t Train time/sample: 800\n",
      "\t Train time/batch_in_epoch: 8\n",
      "\t Train time/sample_in_epoch: 800\n",
      "\t Train memory/current_allocated_mem: 2.4393\n",
      "\t Train memory/current_active_mem: 2.4393\n",
      "\t Train memory/current_inactive_mem: 4.6239\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 200.1112\n",
      "\t Train metrics/train/MSE: 198.1206\n",
      "\t Train metrics/train/MVC: 202.1018\n",
      "\t Train time/train: 0.0755\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0755\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0316\n",
      "[batch=10/30]:\n",
      "\t Train time/batch: 9\n",
      "\t Train time/sample: 900\n",
      "\t Train time/batch_in_epoch: 9\n",
      "\t Train time/sample_in_epoch: 900\n",
      "\t Train memory/current_allocated_mem: 2.4434\n",
      "\t Train memory/current_active_mem: 2.4434\n",
      "\t Train memory/current_inactive_mem: 3.8879\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 190.4424\n",
      "\t Train metrics/train/MSE: 187.8636\n",
      "\t Train metrics/train/MVC: 193.0212\n",
      "\t Train time/train: 0.0894\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0894\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0577\n",
      "[batch=11/30]:\n",
      "\t Train time/batch: 10\n",
      "\t Train time/sample: 1000\n",
      "\t Train time/batch_in_epoch: 10\n",
      "\t Train time/sample_in_epoch: 1000\n",
      "\t Train memory/current_allocated_mem: 2.4431\n",
      "\t Train memory/current_active_mem: 2.4431\n",
      "\t Train memory/current_inactive_mem: 6.1846\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 195.7064\n",
      "\t Train metrics/train/MSE: 193.5939\n",
      "\t Train metrics/train/MVC: 197.8189\n",
      "\t Train time/train: 0.0895\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0895\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0495\n",
      "[batch=12/30]:\n",
      "\t Train time/batch: 11\n",
      "\t Train time/sample: 1100\n",
      "\t Train time/batch_in_epoch: 11\n",
      "\t Train time/sample_in_epoch: 1100\n",
      "\t Train memory/current_allocated_mem: 2.4415\n",
      "\t Train memory/current_active_mem: 2.4415\n",
      "\t Train memory/current_inactive_mem: 4.9383\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 190.3260\n",
      "\t Train metrics/train/MSE: 188.2285\n",
      "\t Train metrics/train/MVC: 192.4234\n",
      "\t Train time/train: 0.0896\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0896\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0428\n",
      "[batch=13/30]:\n",
      "\t Train time/batch: 12\n",
      "\t Train time/sample: 1200\n",
      "\t Train time/batch_in_epoch: 12\n",
      "\t Train time/sample_in_epoch: 1200\n",
      "\t Train memory/current_allocated_mem: 2.4400\n",
      "\t Train memory/current_active_mem: 2.4400\n",
      "\t Train memory/current_inactive_mem: 3.3670\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 189.1282\n",
      "\t Train metrics/train/MSE: 187.0600\n",
      "\t Train metrics/train/MVC: 191.1965\n",
      "\t Train time/train: 0.0896\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0896\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0372\n",
      "[batch=14/30]:\n",
      "\t Train time/batch: 13\n",
      "\t Train time/sample: 1300\n",
      "\t Train time/batch_in_epoch: 13\n",
      "\t Train time/sample_in_epoch: 1300\n",
      "\t Train memory/current_allocated_mem: 2.4393\n",
      "\t Train memory/current_active_mem: 2.4393\n",
      "\t Train memory/current_inactive_mem: 3.2104\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 192.1170\n",
      "\t Train metrics/train/MSE: 190.2350\n",
      "\t Train metrics/train/MVC: 193.9991\n",
      "\t Train time/train: 0.0897\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0897\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0324\n",
      "[batch=15/30]:\n",
      "\t Train time/batch: 14\n",
      "\t Train time/sample: 1400\n",
      "\t Train time/batch_in_epoch: 14\n",
      "\t Train time/sample_in_epoch: 1400\n",
      "\t Train memory/current_allocated_mem: 2.4414\n",
      "\t Train memory/current_active_mem: 2.4414\n",
      "\t Train memory/current_inactive_mem: 4.6260\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 198.7031\n",
      "\t Train metrics/train/MSE: 196.2748\n",
      "\t Train metrics/train/MVC: 201.1314\n",
      "\t Train time/train: 0.0898\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0898\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0283\n",
      "[batch=16/30]:\n",
      "\t Train time/batch: 15\n",
      "\t Train time/sample: 1500\n",
      "\t Train time/batch_in_epoch: 15\n",
      "\t Train time/sample_in_epoch: 1500\n",
      "\t Train memory/current_allocated_mem: 2.4481\n",
      "\t Train memory/current_active_mem: 2.4481\n",
      "\t Train memory/current_inactive_mem: 5.2463\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 188.2001\n",
      "\t Train metrics/train/MSE: 185.9743\n",
      "\t Train metrics/train/MVC: 190.4259\n",
      "\t Train time/train: 0.0899\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0899\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0247\n",
      "[batch=17/30]:\n",
      "\t Train time/batch: 16\n",
      "\t Train time/sample: 1600\n",
      "\t Train time/batch_in_epoch: 16\n",
      "\t Train time/sample_in_epoch: 1600\n",
      "\t Train memory/current_allocated_mem: 2.4399\n",
      "\t Train memory/current_active_mem: 2.4399\n",
      "\t Train memory/current_inactive_mem: 4.9400\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 189.4178\n",
      "\t Train metrics/train/MSE: 186.9411\n",
      "\t Train metrics/train/MVC: 191.8944\n",
      "\t Train time/train: 0.0902\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0902\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0218\n",
      "[batch=18/30]:\n",
      "\t Train time/batch: 17\n",
      "\t Train time/sample: 1700\n",
      "\t Train time/batch_in_epoch: 17\n",
      "\t Train time/sample_in_epoch: 1700\n",
      "\t Train memory/current_allocated_mem: 2.4444\n",
      "\t Train memory/current_active_mem: 2.4444\n",
      "\t Train memory/current_inactive_mem: 3.4695\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 191.6159\n",
      "\t Train metrics/train/MSE: 189.5291\n",
      "\t Train metrics/train/MVC: 193.7026\n",
      "\t Train time/train: 0.0936\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0936\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0213\n",
      "[batch=19/30]:\n",
      "\t Train time/batch: 18\n",
      "\t Train time/sample: 1800\n",
      "\t Train time/batch_in_epoch: 18\n",
      "\t Train time/sample_in_epoch: 1800\n",
      "\t Train memory/current_allocated_mem: 2.4409\n",
      "\t Train memory/current_active_mem: 2.4409\n",
      "\t Train memory/current_inactive_mem: 3.7876\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 189.5534\n",
      "\t Train metrics/train/MSE: 187.4367\n",
      "\t Train metrics/train/MVC: 191.6702\n",
      "\t Train time/train: 0.0937\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0937\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0185\n",
      "[batch=20/30]:\n",
      "\t Train time/batch: 19\n",
      "\t Train time/sample: 1900\n",
      "\t Train time/batch_in_epoch: 19\n",
      "\t Train time/sample_in_epoch: 1900\n",
      "\t Train memory/current_allocated_mem: 2.4400\n",
      "\t Train memory/current_active_mem: 2.4400\n",
      "\t Train memory/current_inactive_mem: 4.6253\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 185.1514\n",
      "\t Train metrics/train/MSE: 183.1042\n",
      "\t Train metrics/train/MVC: 187.1986\n",
      "\t Train time/train: 0.0938\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0938\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0160\n",
      "[batch=21/30]:\n",
      "\t Train time/batch: 20\n",
      "\t Train time/sample: 2000\n",
      "\t Train time/batch_in_epoch: 20\n",
      "\t Train time/sample_in_epoch: 2000\n",
      "\t Train memory/current_allocated_mem: 2.4398\n",
      "\t Train memory/current_active_mem: 2.4398\n",
      "\t Train memory/current_inactive_mem: 4.6255\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 193.9183\n",
      "\t Train metrics/train/MSE: 191.2772\n",
      "\t Train metrics/train/MVC: 196.5594\n",
      "\t Train throughput/batches_per_sec: 0.1811\n",
      "\t Train throughput/samples_per_sec: 18.1063\n",
      "\t Train throughput/device/batches_per_sec: 0.1811\n",
      "\t Train throughput/device/samples_per_sec: 18.1063\n",
      "\t Train throughput/flops_per_sec: 9297874284842.6992\n",
      "\t Train throughput/device/flops_per_sec: 9297874284842.6992\n",
      "\t Train throughput/device/mfu: 0.0298\n",
      "\t Train time/train: 0.0939\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0939\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0137\n",
      "[batch=22/30]:\n",
      "\t Train time/batch: 21\n",
      "\t Train time/sample: 2100\n",
      "\t Train time/batch_in_epoch: 21\n",
      "\t Train time/sample_in_epoch: 2100\n",
      "\t Train memory/current_allocated_mem: 2.4388\n",
      "\t Train memory/current_active_mem: 2.4388\n",
      "\t Train memory/current_inactive_mem: 4.6286\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 192.3468\n",
      "\t Train metrics/train/MSE: 190.3125\n",
      "\t Train metrics/train/MVC: 194.3811\n",
      "\t Train throughput/batches_per_sec: 0.1826\n",
      "\t Train throughput/samples_per_sec: 18.2555\n",
      "\t Train throughput/device/batches_per_sec: 0.1826\n",
      "\t Train throughput/device/samples_per_sec: 18.2555\n",
      "\t Train throughput/flops_per_sec: 9374451717560.0156\n",
      "\t Train throughput/device/flops_per_sec: 9374451717560.0156\n",
      "\t Train throughput/device/mfu: 0.0300\n",
      "\t Train time/train: 0.0939\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0939\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0116\n",
      "[batch=23/30]:\n",
      "\t Train time/batch: 22\n",
      "\t Train time/sample: 2200\n",
      "\t Train time/batch_in_epoch: 22\n",
      "\t Train time/sample_in_epoch: 2200\n",
      "\t Train memory/current_allocated_mem: 2.4406\n",
      "\t Train memory/current_active_mem: 2.4406\n",
      "\t Train memory/current_inactive_mem: 3.3685\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 187.4770\n",
      "\t Train metrics/train/MSE: 185.0686\n",
      "\t Train metrics/train/MVC: 189.8855\n",
      "\t Train throughput/batches_per_sec: 0.1825\n",
      "\t Train throughput/samples_per_sec: 18.2548\n",
      "\t Train throughput/device/batches_per_sec: 0.1825\n",
      "\t Train throughput/device/samples_per_sec: 18.2548\n",
      "\t Train throughput/flops_per_sec: 9374112027020.3164\n",
      "\t Train throughput/device/flops_per_sec: 9374112027020.3164\n",
      "\t Train throughput/device/mfu: 0.0300\n",
      "\t Train time/train: 0.0940\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0940\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0097\n",
      "[batch=24/30]:\n",
      "\t Train time/batch: 23\n",
      "\t Train time/sample: 2300\n",
      "\t Train time/batch_in_epoch: 23\n",
      "\t Train time/sample_in_epoch: 2300\n",
      "\t Train memory/current_allocated_mem: 2.4454\n",
      "\t Train memory/current_active_mem: 2.4454\n",
      "\t Train memory/current_inactive_mem: 5.1442\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 187.3961\n",
      "\t Train metrics/train/MSE: 184.9071\n",
      "\t Train metrics/train/MVC: 189.8852\n",
      "\t Train throughput/batches_per_sec: 0.1825\n",
      "\t Train throughput/samples_per_sec: 18.2535\n",
      "\t Train throughput/device/batches_per_sec: 0.1825\n",
      "\t Train throughput/device/samples_per_sec: 18.2535\n",
      "\t Train throughput/flops_per_sec: 9373448974102.1875\n",
      "\t Train throughput/device/flops_per_sec: 9373448974102.1875\n",
      "\t Train throughput/device/mfu: 0.0300\n",
      "\t Train time/train: 0.0941\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0941\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0080\n",
      "[batch=25/30]:\n",
      "\t Train time/batch: 24\n",
      "\t Train time/sample: 2400\n",
      "\t Train time/batch_in_epoch: 24\n",
      "\t Train time/sample_in_epoch: 2400\n",
      "\t Train memory/current_allocated_mem: 2.4376\n",
      "\t Train memory/current_active_mem: 2.4376\n",
      "\t Train memory/current_inactive_mem: 3.8937\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 190.0456\n",
      "\t Train metrics/train/MSE: 187.6189\n",
      "\t Train metrics/train/MVC: 192.4722\n",
      "\t Train throughput/batches_per_sec: 0.1825\n",
      "\t Train throughput/samples_per_sec: 18.2538\n",
      "\t Train throughput/device/batches_per_sec: 0.1825\n",
      "\t Train throughput/device/samples_per_sec: 18.2538\n",
      "\t Train throughput/flops_per_sec: 9373598858633.6914\n",
      "\t Train throughput/device/flops_per_sec: 9373598858633.6914\n",
      "\t Train throughput/device/mfu: 0.0300\n",
      "\t Train time/train: 0.0942\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0942\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0064\n",
      "[batch=26/30]:\n",
      "\t Train time/batch: 25\n",
      "\t Train time/sample: 2500\n",
      "\t Train time/batch_in_epoch: 25\n",
      "\t Train time/sample_in_epoch: 2500\n",
      "\t Train memory/current_allocated_mem: 2.4483\n",
      "\t Train memory/current_active_mem: 2.4483\n",
      "\t Train memory/current_inactive_mem: 5.1434\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 188.8573\n",
      "\t Train metrics/train/MSE: 186.5336\n",
      "\t Train metrics/train/MVC: 191.1810\n",
      "\t Train throughput/batches_per_sec: 0.1745\n",
      "\t Train throughput/samples_per_sec: 17.4471\n",
      "\t Train throughput/device/batches_per_sec: 0.1745\n",
      "\t Train throughput/device/samples_per_sec: 17.4471\n",
      "\t Train throughput/flops_per_sec: 8959360652371.8379\n",
      "\t Train throughput/device/flops_per_sec: 8959360652371.8379\n",
      "\t Train throughput/device/mfu: 0.0287\n",
      "\t Train time/train: 0.0957\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0957\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0052\n",
      "[batch=27/30]:\n",
      "\t Train time/batch: 26\n",
      "\t Train time/sample: 2600\n",
      "\t Train time/batch_in_epoch: 26\n",
      "\t Train time/sample_in_epoch: 2600\n",
      "\t Train memory/current_allocated_mem: 2.4428\n",
      "\t Train memory/current_active_mem: 2.4428\n",
      "\t Train memory/current_inactive_mem: 5.2516\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 189.2632\n",
      "\t Train metrics/train/MSE: 186.9216\n",
      "\t Train metrics/train/MVC: 191.6047\n",
      "\t Train throughput/batches_per_sec: 0.1745\n",
      "\t Train throughput/samples_per_sec: 17.4467\n",
      "\t Train throughput/device/batches_per_sec: 0.1745\n",
      "\t Train throughput/device/samples_per_sec: 17.4467\n",
      "\t Train throughput/flops_per_sec: 8959143067021.7266\n",
      "\t Train throughput/device/flops_per_sec: 8959143067021.7266\n",
      "\t Train throughput/device/mfu: 0.0287\n",
      "\t Train time/train: 0.0958\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0958\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0037\n",
      "[batch=28/30]:\n",
      "\t Train time/batch: 27\n",
      "\t Train time/sample: 2700\n",
      "\t Train time/batch_in_epoch: 27\n",
      "\t Train time/sample_in_epoch: 2700\n",
      "\t Train memory/current_allocated_mem: 2.4400\n",
      "\t Train memory/current_active_mem: 2.4400\n",
      "\t Train memory/current_inactive_mem: 3.7864\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 190.9627\n",
      "\t Train metrics/train/MSE: 188.4980\n",
      "\t Train metrics/train/MVC: 193.4274\n",
      "\t Train throughput/batches_per_sec: 0.1886\n",
      "\t Train throughput/samples_per_sec: 18.8637\n",
      "\t Train throughput/device/batches_per_sec: 0.1886\n",
      "\t Train throughput/device/samples_per_sec: 18.8637\n",
      "\t Train throughput/flops_per_sec: 9686786628657.9648\n",
      "\t Train throughput/device/flops_per_sec: 9686786628657.9648\n",
      "\t Train throughput/device/mfu: 0.0310\n",
      "\t Train time/train: 0.0959\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0959\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0024\n",
      "[batch=29/30]:\n",
      "\t Train time/batch: 28\n",
      "\t Train time/sample: 2800\n",
      "\t Train time/batch_in_epoch: 28\n",
      "\t Train time/sample_in_epoch: 2800\n",
      "\t Train memory/current_allocated_mem: 2.4402\n",
      "\t Train memory/current_active_mem: 2.4402\n",
      "\t Train memory/current_inactive_mem: 3.4738\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 191.0959\n",
      "\t Train metrics/train/MSE: 189.2506\n",
      "\t Train metrics/train/MVC: 192.9412\n",
      "\t Train throughput/batches_per_sec: 0.2706\n",
      "\t Train throughput/samples_per_sec: 27.0577\n",
      "\t Train throughput/device/batches_per_sec: 0.2706\n",
      "\t Train throughput/device/samples_per_sec: 27.0577\n",
      "\t Train throughput/flops_per_sec: 13894555972963.3555\n",
      "\t Train throughput/device/flops_per_sec: 13894555972963.3555\n",
      "\t Train throughput/device/mfu: 0.0445\n",
      "\t Train time/train: 0.0960\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0960\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0012\n",
      "[batch=30/30]:\n",
      "\t Train time/batch: 29\n",
      "\t Train time/sample: 2900\n",
      "\t Train time/batch_in_epoch: 29\n",
      "\t Train time/sample_in_epoch: 2900\n",
      "\t Train memory/current_allocated_mem: 2.4414\n",
      "\t Train memory/current_active_mem: 2.4414\n",
      "\t Train memory/current_inactive_mem: 4.6239\n",
      "\t Train memory/current_reserved_mem: 21.8120\n",
      "\t Train memory/peak_allocated_mem: 40.4660\n",
      "\t Train memory/peak_active_mem: 40.4660\n",
      "\t Train memory/peak_inactive_mem: 6.7807\n",
      "\t Train memory/peak_reserved_mem: 41.2530\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 100\n",
      "\t Train loss/train/total: 188.9500\n",
      "\t Train metrics/train/MSE: 186.7888\n",
      "\t Train metrics/train/MVC: 191.1112\n",
      "\t Train throughput/batches_per_sec: 0.8245\n",
      "\t Train throughput/samples_per_sec: 82.4523\n",
      "\t Train throughput/device/batches_per_sec: 0.8245\n",
      "\t Train throughput/device/samples_per_sec: 82.4523\n",
      "\t Train throughput/flops_per_sec: 42340488613666.8047\n",
      "\t Train throughput/device/flops_per_sec: 42340488613666.8047\n",
      "\t Train throughput/device/mfu: 0.1357\n",
      "\t Train time/train: 0.0961\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0961\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0000\n",
      "[Eval batch=1/100] Eval on eval data\n",
      "[Eval batch=11/100] Eval on eval data\n",
      "[Eval batch=21/100] Eval on eval data\n",
      "[Eval batch=31/100] Eval on eval data\n",
      "[Eval batch=41/100] Eval on eval data\n",
      "[Eval batch=50/100] Eval on eval data\n",
      "[Eval batch=60/100] Eval on eval data\n",
      "[Eval batch=70/100] Eval on eval data\n",
      "[Eval batch=80/100] Eval on eval data\n",
      "[Eval batch=90/100] Eval on eval data\n",
      "[Eval batch=100/100] Eval on eval data:\n",
      "\t Eval metrics/eval/MSE: 188.8645\n",
      "\t Eval metrics/eval/MVC: 193.3662\n",
      "\t Eval metrics/eval/Spearman: 0.5743\n",
      "2025-10-21 21:44:29,246: rank0[325359][MainThread]: INFO: train: Training finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed!\n"
     ]
    }
   ],
   "source": [
    "# Start fine-tuning\n",
    "finetune_trainer = main(finetune_cfg)\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "\n",
    "1. You can monitor your training in **Weights & Biases**\n",
    "2. Model checkpoints are saved according to `save_interval`\n",
    "3. IF you encounter OOM issues try reducing the `device_train_batch_size`\n",
    "4. If you are using a single GPU you can remove the `fsdp_config` from your custom configuration.\n",
    "5. Ensure `attn_impl: flash` and `use_attn_mask: False` as Triton backend is no longer supported by our codebase (Email us if you have questions on how to use triton backend with custom attn masking)\n",
    "6. You can add `cell_classification` and `marginal_essentiality` callbacks to the configuration files so that the model will authomathically be evaluated on these benchmarks. (adding some samples is TODO)\n",
    "\n",
    "7. After training you can:\n",
    "    1. **Prepare model for inference**: Use `scripts/prepare_for_inference.py`\n",
    "    2. **Extract cell and gene embeddings**: See `scripts/clustering_tutorial.ipynb` and `inference.predict_embeddings`\n",
    "    3. **Run benchmarks**: See `scripts/depmap/` and `scripts/msigdb/`\n",
    "    4. **Upload to HuggingFace**: For sharing your trained model\n",
    "\n",
    "For more details, refer to the [README.md](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahoe-x1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
