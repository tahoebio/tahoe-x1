{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahoe-x1 Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to train a Tahoe-x1 model from scratch or fine-tune a pre-trained model.\n",
    "\n",
    "### 0. Prerequisites\n",
    "- Access to GPU resources (NVIDIA H100/H200 recommended)\n",
    "- Tahoe-x1 package installed (Refer to README for the installation guide)\n",
    "- Access to training data (see README for dataset information) \n",
    "    - You either need to have the training data locally in your machine or provide the aws s3 credentials so that the data can be strimmed from our public s3 bucket (recommended)\n",
    "- Weights & Biases account (optional, for logging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Customize Config\n",
    "\n",
    "You can start with the `test_run.yaml` which is a sample config on how training the 70M model  and customize it for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoex\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0003\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-05\n",
      "scheduler:\n",
      "  name: cosine_with_warmup\n",
      "  t_warmup: 0.05dur\n",
      "  t_max: 1dur\n",
      "  alpha_f: 0.1\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 6ep\n",
      "fsdp_config:\n",
      "  sharding_strategy: FULL_SHARD\n",
      "  mixed_precision: DEFAULT\n",
      "  activation_checkpointing: false\n",
      "  activation_checkpointing_reentrant: false\n",
      "  activation_cpu_offload: false\n",
      "  limit_all_gathers: true\n",
      "  verbose: true\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoex\n",
      "    log_artifacts: false\n",
      "save_folder: s3://tahoe-hackathon-data/MFM/ckpts/{run_name}\n",
      "save_interval: 2000ba\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Load the base configuration\n",
    "cfg = om.load(\"../configs/test_run.yaml\")\n",
    "print(om.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to: ./my_training_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Customize the config based on your system, design choice, etc\n",
    "\n",
    "# Training settings\n",
    "cfg.global_train_batch_size = 256  # Total batch size across all devices\n",
    "cfg.max_duration = \"20ba\" #\"2ep\"  # Train for 2 epochs (adjust as needed)\n",
    "\n",
    "# Model configuration\n",
    "cfg.model.d_model = 512\n",
    "cfg.model.n_layers = 12\n",
    "cfg.model.n_heads = 8\n",
    "\n",
    "# IMPORTANT: Current codebase only supports flash attention without attention mask\n",
    "cfg.model.attn_config.attn_impl = \"flash\"\n",
    "cfg.model.attn_config.use_attn_mask = False\n",
    "\n",
    "# Data loader settings\n",
    "cfg.train_loader.num_workers = 4  # Adjust based on your system\n",
    "cfg.train_loader.prefetch_factor = 2 # Adjust based on your system\n",
    "\n",
    "cfg.collator.use_chem_token=False # You can set it to True if your training data includes drug info(such as Tahoe100M) and you want to inject that to the model\n",
    "# Optimizer settings\n",
    "cfg.optimizer.lr = 3.0e-4\n",
    "cfg.optimizer.weight_decay = 1.0e-05\n",
    "\n",
    "# Logging\n",
    "cfg.run_name = \"custom_test_run\"\n",
    "cfg.loggers.wandb.project = \"tahoe_x1-tutorial\"\n",
    "save_folder = cfg.save_folder = f\"./checkpoints/{cfg.run_name}\"\n",
    "cfg.save_interval = \"500ba\"  # Save every 500 batches\n",
    "\n",
    "# Save the config\n",
    "custom_config_path = \"./my_training_config.yaml\"\n",
    "om.save(cfg, custom_config_path)\n",
    "print(f\"Configuration saved to: {custom_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training from Scratch\n",
    "\n",
    "#### Option A: Train using the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import train module from scripts directory\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath('../scripts'))\nfrom train import main\n\ncfg = om.load(custom_config_path)\n\n# Train the model\ntrainer = main(cfg)\n\nprint(f\"Training completed and checkpoints saved at {save_folder}!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Train using Composer CLI\n",
    "\n",
    "Alternatively, you can train using the command line with composer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run training via shell command\n!composer ../scripts/train.py -f {custom_config_path}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Resume Training\n",
    "\n",
    "Note that if your run stopped unexpectedly and you want to resume the training from where it stopped, simply use the **same `run_name` and `save_folder`** in the configuration. The trainer will automatically pick up from the last saved checkpoint.\n",
    "The trainer will automatically detect existing checkpoints and resume with full state (model weights, optimizer, scheduler, etc.).\n",
    "\n",
    "```python\n",
    "resume_cfg = om.load(custom_config_path)\n",
    "\n",
    "# Keep the same run_name and save_folder - training will auto-resume\n",
    "trainer = main(resume_cfg)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tuning a Pre-trained Model\n",
    "\n",
    "When loading from a checkpoint, you have two options:\n",
    "\n",
    "**Option 1:** Full Recovery\n",
    "- Set `load_path` to your checkpoint directory or file\n",
    "- Loads both model weights AND optimizer/scheduler states\n",
    "\n",
    "```python\n",
    "cfg.load_path = \"s3://bucket/path/to/checkpoint/\"\n",
    "# This recovers everything: weights + optimizer + scheduler\n",
    "```\n",
    "\n",
    "**Option 2:** Weights Only \n",
    "- Set `load_path` AND `load_weights_only=True`\n",
    "- Loads **only model weights**, optimizer/scheduler are initialized fresh\n",
    "\n",
    "```python\n",
    "cfg.load_path = \"s3://bucket/path/to/checkpoint/\"\n",
    "cfg.load_weights_only = True\n",
    "# This loads only weights, optimizer/scheduler start fresh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning configuration:\n",
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoex\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 1.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-06\n",
      "scheduler:\n",
      "  name: constant_with_warmup\n",
      "  t_warmup: 0ba\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 30ba\n",
      "fsdp_config:\n",
      "  sharding_strategy: FULL_SHARD\n",
      "  mixed_precision: DEFAULT\n",
      "  activation_checkpointing: false\n",
      "  activation_checkpointing_reentrant: false\n",
      "  activation_cpu_offload: false\n",
      "  limit_all_gathers: true\n",
      "  verbose: true\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoex\n",
      "    log_artifacts: false\n",
      "save_folder: ./checkpoints/finetuned_{run_name}\n",
      "save_interval: 2000ba\n",
      "load_path: s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load configuration for fine-tuning\n",
    "finetune_cfg = om.load(\"../configs/test_run.yaml\")\n",
    "\n",
    "# Set checkpoint path\n",
    "checkpoint_path = \"s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\"  # Or local path\n",
    "finetune_cfg.load_path = checkpoint_path\n",
    "\n",
    "# Adjust learning rate for fine-tuning and schedular for finetuning\n",
    "finetune_cfg.optimizer.lr = 1.0e-5\n",
    "finetune_cfg.optimizer.weight_decay = 1.0e-6\n",
    "finetune_cfg.scheduler = {}\n",
    "finetune_cfg.scheduler.name = 'constant_with_warmup'\n",
    "finetune_cfg.scheduler.t_warmup = '0ba'\n",
    "\n",
    "# Shorter training duration for fine-tuning\n",
    "finetune_cfg.max_duration = \"30ba\"\n",
    "\n",
    "# Update save folder\n",
    "finetune_cfg.save_folder = \"./checkpoints/finetuned_{run_name}\"\n",
    "\n",
    "print(\"Fine-tuning configuration:\")\n",
    "print(om.to_yaml(finetune_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start fine-tuning\n# Note: main was imported in cell 5, but if running this cell standalone, ensure it's imported:\n# import sys, os\n# sys.path.insert(0, os.path.abspath('../scripts'))\n# from train import main\n\nfinetune_trainer = main(finetune_cfg)\nprint(\"Fine-tuning completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "\n",
    "1. You can monitor your training in **Weights & Biases**\n",
    "2. Model checkpoints are saved according to `save_interval`\n",
    "3. IF you encounter OOM issues try reducing the `device_train_batch_size`\n",
    "4. If you are using a single GPU you can remove the `fsdp_config` from your custom configuration.\n",
    "5. Ensure `attn_impl: flash` and `use_attn_mask: False` as Triton backend is no longer supported by our codebase (Email us if you have questions on how to use triton backend with custom attn masking)\n",
    "6. You can add `cell_classification` and `marginal_essentiality` callbacks to the configuration files so that the model will authomathically be evaluated on these benchmarks. (adding some samples is TODO)\n",
    "\n",
    "7. After training you can:\n",
    "    1. **Prepare model for inference**: Use `scripts/prepare_for_inference.py`\n",
    "    2. **Extract cell and gene embeddings**: See `clustering_tutorial.ipynb` and `inference.predict_embeddings`\n",
    "    3. **Run benchmarks**: See `scripts/depmap/` and `scripts/msigdb/`\n",
    "    4. **Upload to HuggingFace**: For sharing your trained model\n",
    "\n",
    "For more details, refer to the [README.md](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahoe-x1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
