{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahoe-x1 Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to train a Tahoe-x1 model from scratch or fine-tune a pre-trained model.\n",
    "\n",
    "### 0. Prerequisites\n",
    "- Access to GPU resources (NVIDIA H100/H200 recommended)\n",
    "- Tahoe-x1 package installed (Refer to README for the installation guide)\n",
    "- Access to training data (see README for dataset information) \n",
    "    - You either need to have the training data locally in your machine or provide the aws s3 credentials so that the data can be strimmed from our public s3 bucket (recommended)\n",
    "- Weights & Biases account (optional, for logging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Customize Config\n",
    "\n",
    "You can start with `test_run.yaml`, which is a sample configuration demonstrating how to train the 70M model, and customize it for your training needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoe_x1\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      tahoe:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/tahoe_100m_MDS_v2/train/\n",
      "        local: mds-data-folder/tahoe-100m/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 4\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      tahoe:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/tahoe_100m_MDS_v2/valid/\n",
      "        local: mds-data-folder/tahoe-100m/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 4\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 0.0003\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-05\n",
      "scheduler:\n",
      "  name: cosine_with_warmup\n",
      "  t_warmup: 0.05dur\n",
      "  t_max: 1dur\n",
      "  alpha_f: 0.1\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 10ba\n",
      "fsdp_config:\n",
      "  sharding_strategy: FULL_SHARD\n",
      "  mixed_precision: DEFAULT\n",
      "  activation_checkpointing: false\n",
      "  activation_checkpointing_reentrant: false\n",
      "  activation_cpu_offload: false\n",
      "  limit_all_gathers: true\n",
      "  verbose: true\n",
      "progress_bar: false\n",
      "log_to_console: true\n",
      "console_log_interval: 1ba\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "save_interval: 500ba\n",
      "save_num_checkpoints_to_keep: 1\n",
      "save_folder: ./{run_name}/checkpoints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Load the base configuration\n",
    "cfg = om.load(\"../configs/test_run.yaml\")\n",
    "print(om.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to: ./my_training_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Customize the config based on your system, design choice, etc\n",
    "\n",
    "# Training settings\n",
    "cfg.global_train_batch_size = 256  # Total batch size across all devices\n",
    "cfg.max_duration = \"20ba\" #\"2ep\"  # Train for 2 epochs (adjust as needed)\n",
    "\n",
    "# Model configuration\n",
    "cfg.model.d_model = 512\n",
    "cfg.model.n_layers = 12\n",
    "cfg.model.n_heads = 8\n",
    "\n",
    "# IMPORTANT: Current codebase only supports flash attention without attention mask\n",
    "cfg.model.attn_config.attn_impl = \"flash\"\n",
    "cfg.model.attn_config.use_attn_mask = False\n",
    "\n",
    "# Data loader settings\n",
    "cfg.train_loader.num_workers = 4  # Adjust based on your system\n",
    "cfg.train_loader.prefetch_factor = 2 # Adjust based on your system\n",
    "\n",
    "cfg.collator.use_chem_token=False # You can set it to True if your training data includes drug info(such as Tahoe100M) and you want to inject that to the model\n",
    "# Optimizer settings\n",
    "cfg.optimizer.lr = 3.0e-4\n",
    "cfg.optimizer.weight_decay = 1.0e-05\n",
    "\n",
    "# Logging\n",
    "cfg.run_name = \"custom_test_run\"\n",
    "# cfg.loggers.wandb.project = \"tahoex-tutorial\"\n",
    "save_folder = cfg.save_folder = f\"./checkpoints/{cfg.run_name}\"\n",
    "cfg.save_interval = \"500ba\"  # Save every 500 batches\n",
    "\n",
    "# Save the config\n",
    "custom_config_path = \"./my_training_config.yaml\"\n",
    "om.save(cfg, custom_config_path)\n",
    "print(f\"Configuration saved to: {custom_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training from Scratch\n",
    "\n",
    "#### Option A: Train using the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/llmfoundry/callbacks/env_logging_callback.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/tahoe/tahoe-x1/scripts/train.py:341: UserWarning: FSDP is not applicable for single-GPU training. Reverting to DDP.\n",
      "  warnings.warn(\n",
      "2025-10-23 21:13:21,952: rank0[427769][MainThread]: INFO: train: Downloading vocab...\n",
      "2025-10-23 21:13:22,056: rank0[427769][MainThread]: DEBUG: tahoe_x1.utils.s3_utils: Failed to download s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json (signed): Unable to locate credentials\n",
      "2025-10-23 21:13:24,681: rank0[427769][MainThread]: INFO: tahoe_x1.utils.s3_utils: Successfully downloaded s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json (unsigned) to /tahoe/tahoe-x1/tutorials/vocab.json\n",
      "[rank0]:[W1023 21:13:24.007049027 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "2025-10-23 21:13:25,112: rank0[427769][MainThread]: INFO: train: Setting vocab size to: 62720\n",
      "2025-10-23 21:13:25,122: rank0[427769][MainThread]: INFO: train: Building DataLoaders...\n",
      "2025-10-23 21:13:28,003: rank0[427769][MainThread]: INFO: train: train set number of samples: 94668090\n",
      "2025-10-23 21:13:28,052: rank0[427769][MainThread]: INFO: train: Validation set number of samples: 956244\n",
      "2025-10-23 21:13:28,384: rank0[427769][MainThread]: INFO: tahoe_x1.model.model: MosaicML recommends using config.init_device=\"meta\" with Composer + FSDP for faster initialization.\n",
      "2025-10-23 21:13:29,499: rank0[427769][MainThread]: INFO: train: Total parameters: 70.996993 M\n",
      "2025-10-23 21:13:29,500: rank0[427769][MainThread]: INFO: train: Total trainable parameters: 70.996993 M \n",
      "2025-10-23 21:13:29,500: rank0[427769][MainThread]: INFO: train: gene_encoder: 32.113664 M parameters\n",
      "2025-10-23 21:13:29,501: rank0[427769][MainThread]: INFO: train: flag_encoder: 0.001024 M parameters\n",
      "2025-10-23 21:13:29,501: rank0[427769][MainThread]: INFO: train: expression_encoder: 0.264704 M parameters\n",
      "2025-10-23 21:13:29,502: rank0[427769][MainThread]: INFO: train: transformer_encoder: 37.829632 M parameters\n",
      "2025-10-23 21:13:29,503: rank0[427769][MainThread]: INFO: train: expression_decoder: 0.263169 M parameters\n",
      "2025-10-23 21:13:29,503: rank0[427769][MainThread]: INFO: train: mvc_decoder: 0.5248 M parameters\n",
      "2025-10-23 21:13:29,505: rank0[427769][MainThread]: INFO: train: Building Trainer...\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/trainer/trainer.py:247: UserWarning: `device_train_microbatch_size='auto'` may potentially fail with unexpected CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory errors and adjust the batch size, but it is possible CUDA will be put into an irrecoverable state due to PyTorch bugs, e.g. integer overflow. In this case, please manually set device_train_microbatch_size explicitly to an integer instead.\n",
      "  warnings.warn((\n",
      "2025-10-23 21:13:30,717: rank0[427769][MainThread]: INFO: train: Logging config\n",
      "2025-10-23 21:13:31,061: rank0[427769][MainThread]: INFO: train: Starting training...\n",
      "******************************\n",
      "Config:\n",
      "composer_commit_hash: None\n",
      "composer_version: 0.28.0\n",
      "enabled_algorithms/GradientClipping: true\n",
      "enabled_algorithms/LowPrecisionLayerNorm: true\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 777\n",
      "time/remaining_estimate_unit: hours\n",
      "\n",
      "******************************\n",
      "/tahoe/tahoe-x1/.venv/lib/python3.10/site-packages/composer/trainer/trainer.py:368: RuntimeWarning: CUDA out of memory or excessive memory allocation retries detected. Train microbatch size will be decreased from 256 -> 128.\n",
      "  warnings.warn(\n",
      "[batch=1/20]:\n",
      "\t Train time/epoch: 0\n",
      "\t Train time/batch: 0\n",
      "\t Train time/sample: 0\n",
      "\t Train time/batch_in_epoch: 0\n",
      "\t Train time/sample_in_epoch: 0\n",
      "\t Train memory/current_allocated_mem: 0.8658\n",
      "\t Train memory/current_active_mem: 0.8658\n",
      "\t Train memory/current_inactive_mem: 5.6563\n",
      "\t Train memory/current_reserved_mem: 26.8710\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 5.9159\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 385.4410\n",
      "\t Train metrics/train/MSE: 371.6417\n",
      "\t Train metrics/train/MVC: 399.2401\n",
      "\t Train time/train: 0.0342\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0342\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "[batch=2/20]:\n",
      "\t Train time/batch: 1\n",
      "\t Train time/sample: 256\n",
      "\t Train time/batch_in_epoch: 1\n",
      "\t Train time/sample_in_epoch: 256\n",
      "\t Train memory/current_allocated_mem: 1.4283\n",
      "\t Train memory/current_active_mem: 1.4283\n",
      "\t Train memory/current_inactive_mem: 9.8585\n",
      "\t Train memory/current_reserved_mem: 26.9020\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 390.5784\n",
      "\t Train metrics/train/MSE: 376.8949\n",
      "\t Train metrics/train/MVC: 404.3372\n",
      "\t Train time/train: 0.0383\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0383\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0003\n",
      "\t Train time/remaining_estimate: 0.0042\n",
      "[batch=3/20]:\n",
      "\t Train time/batch: 2\n",
      "\t Train time/sample: 512\n",
      "\t Train time/batch_in_epoch: 2\n",
      "\t Train time/sample_in_epoch: 512\n",
      "\t Train memory/current_allocated_mem: 1.4298\n",
      "\t Train memory/current_active_mem: 1.4298\n",
      "\t Train memory/current_inactive_mem: 5.9689\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 393.2471\n",
      "\t Train metrics/train/MSE: 379.2466\n",
      "\t Train metrics/train/MVC: 407.3843\n",
      "\t Train time/train: 0.0386\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0386\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0003\n",
      "\t Train time/remaining_estimate: 0.0039\n",
      "[batch=4/20]:\n",
      "\t Train time/batch: 3\n",
      "\t Train time/sample: 768\n",
      "\t Train time/batch_in_epoch: 3\n",
      "\t Train time/sample_in_epoch: 768\n",
      "\t Train memory/current_allocated_mem: 1.4325\n",
      "\t Train memory/current_active_mem: 1.4325\n",
      "\t Train memory/current_inactive_mem: 6.9057\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 307.1208\n",
      "\t Train metrics/train/MSE: 317.6149\n",
      "\t Train metrics/train/MVC: 296.6306\n",
      "\t Train time/train: 0.0388\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0388\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0003\n",
      "\t Train time/remaining_estimate: 0.0037\n",
      "[batch=5/20]:\n",
      "\t Train time/batch: 4\n",
      "\t Train time/sample: 1024\n",
      "\t Train time/batch_in_epoch: 4\n",
      "\t Train time/sample_in_epoch: 1024\n",
      "\t Train memory/current_allocated_mem: 1.4322\n",
      "\t Train memory/current_active_mem: 1.4322\n",
      "\t Train memory/current_inactive_mem: 5.9645\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 293.7288\n",
      "\t Train metrics/train/MSE: 295.6598\n",
      "\t Train metrics/train/MVC: 291.8032\n",
      "\t Train time/train: 0.0477\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0477\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0003\n",
      "\t Train time/remaining_estimate: 0.0359\n",
      "[batch=6/20]:\n",
      "\t Train time/batch: 5\n",
      "\t Train time/sample: 1280\n",
      "\t Train time/batch_in_epoch: 5\n",
      "\t Train time/sample_in_epoch: 1280\n",
      "\t Train memory/current_allocated_mem: 1.4309\n",
      "\t Train memory/current_active_mem: 1.4309\n",
      "\t Train memory/current_inactive_mem: 6.7731\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 290.4792\n",
      "\t Train metrics/train/MSE: 289.8818\n",
      "\t Train metrics/train/MVC: 291.0732\n",
      "\t Train time/train: 0.0517\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0517\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0003\n",
      "\t Train time/remaining_estimate: 0.0381\n",
      "[batch=7/20]:\n",
      "\t Train time/batch: 6\n",
      "\t Train time/sample: 1536\n",
      "\t Train time/batch_in_epoch: 6\n",
      "\t Train time/sample_in_epoch: 1536\n",
      "\t Train memory/current_allocated_mem: 1.4272\n",
      "\t Train memory/current_active_mem: 1.4272\n",
      "\t Train memory/current_inactive_mem: 5.1642\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 298.4571\n",
      "\t Train metrics/train/MSE: 297.6569\n",
      "\t Train metrics/train/MVC: 299.3060\n",
      "\t Train time/train: 0.0519\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0519\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0003\n",
      "\t Train time/remaining_estimate: 0.0299\n",
      "[batch=8/20]:\n",
      "\t Train time/batch: 7\n",
      "\t Train time/sample: 1792\n",
      "\t Train time/batch_in_epoch: 7\n",
      "\t Train time/sample_in_epoch: 1792\n",
      "\t Train memory/current_allocated_mem: 1.4285\n",
      "\t Train memory/current_active_mem: 1.4285\n",
      "\t Train memory/current_inactive_mem: 5.9681\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 294.5941\n",
      "\t Train metrics/train/MSE: 294.7499\n",
      "\t Train metrics/train/MVC: 294.4184\n",
      "\t Train time/train: 0.0521\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0521\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "\t Train time/remaining_estimate: 0.0241\n",
      "[batch=9/20]:\n",
      "\t Train time/batch: 8\n",
      "\t Train time/sample: 2048\n",
      "\t Train time/batch_in_epoch: 8\n",
      "\t Train time/sample_in_epoch: 2048\n",
      "\t Train memory/current_allocated_mem: 1.4294\n",
      "\t Train memory/current_active_mem: 1.4294\n",
      "\t Train memory/current_inactive_mem: 5.3003\n",
      "\t Train memory/current_reserved_mem: 27.4390\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 294.6768\n",
      "\t Train metrics/train/MSE: 295.1537\n",
      "\t Train metrics/train/MVC: 294.3336\n",
      "\t Train time/train: 0.0524\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0524\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "\t Train time/remaining_estimate: 0.0196\n",
      "[batch=10/20]:\n",
      "\t Train time/batch: 9\n",
      "\t Train time/sample: 2304\n",
      "\t Train time/batch_in_epoch: 9\n",
      "\t Train time/sample_in_epoch: 2304\n",
      "\t Train memory/current_allocated_mem: 1.4325\n",
      "\t Train memory/current_active_mem: 1.4325\n",
      "\t Train memory/current_inactive_mem: 7.0379\n",
      "\t Train memory/current_reserved_mem: 27.9760\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 295.3487\n",
      "\t Train metrics/train/MSE: 295.5539\n",
      "\t Train metrics/train/MVC: 295.1443\n",
      "\t Train time/train: 0.0526\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0526\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "\t Train time/remaining_estimate: 0.0161\n",
      "[batch=11/20]:\n",
      "\t Train time/batch: 10\n",
      "\t Train time/sample: 2560\n",
      "\t Train time/batch_in_epoch: 10\n",
      "\t Train time/sample_in_epoch: 2560\n",
      "\t Train memory/current_allocated_mem: 1.4288\n",
      "\t Train memory/current_active_mem: 1.4288\n",
      "\t Train memory/current_inactive_mem: 5.9700\n",
      "\t Train memory/current_reserved_mem: 27.9760\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 294.6254\n",
      "\t Train metrics/train/MSE: 295.3130\n",
      "\t Train metrics/train/MVC: 294.0764\n",
      "\t Train time/train: 0.0534\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0534\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "\t Train time/remaining_estimate: 0.0138\n",
      "[batch=12/20]:\n",
      "\t Train time/batch: 11\n",
      "\t Train time/sample: 2816\n",
      "\t Train time/batch_in_epoch: 11\n",
      "\t Train time/sample_in_epoch: 2816\n",
      "\t Train memory/current_allocated_mem: 1.4387\n",
      "\t Train memory/current_active_mem: 1.4387\n",
      "\t Train memory/current_inactive_mem: 5.9600\n",
      "\t Train memory/current_reserved_mem: 27.9760\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 289.0754\n",
      "\t Train metrics/train/MSE: 289.7111\n",
      "\t Train metrics/train/MVC: 288.4799\n",
      "\t Train time/train: 0.0536\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0536\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0002\n",
      "\t Train time/remaining_estimate: 0.0113\n",
      "[batch=13/20]:\n",
      "\t Train time/batch: 12\n",
      "\t Train time/sample: 3072\n",
      "\t Train time/batch_in_epoch: 12\n",
      "\t Train time/sample_in_epoch: 3072\n",
      "\t Train memory/current_allocated_mem: 1.4287\n",
      "\t Train memory/current_active_mem: 1.4287\n",
      "\t Train memory/current_inactive_mem: 5.4290\n",
      "\t Train memory/current_reserved_mem: 27.9760\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 289.9169\n",
      "\t Train metrics/train/MSE: 290.6095\n",
      "\t Train metrics/train/MVC: 289.2980\n",
      "\t Train time/train: 0.0539\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0539\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0092\n",
      "[batch=14/20]:\n",
      "\t Train time/batch: 13\n",
      "\t Train time/sample: 3328\n",
      "\t Train time/batch_in_epoch: 13\n",
      "\t Train time/sample_in_epoch: 3328\n",
      "\t Train memory/current_allocated_mem: 1.4296\n",
      "\t Train memory/current_active_mem: 1.4296\n",
      "\t Train memory/current_inactive_mem: 5.9692\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 290.4212\n",
      "\t Train metrics/train/MSE: 291.1736\n",
      "\t Train metrics/train/MVC: 289.6662\n",
      "\t Train time/train: 0.0541\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0541\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0074\n",
      "[batch=15/20]:\n",
      "\t Train time/batch: 14\n",
      "\t Train time/sample: 3584\n",
      "\t Train time/batch_in_epoch: 14\n",
      "\t Train time/sample_in_epoch: 3584\n",
      "\t Train memory/current_allocated_mem: 1.4262\n",
      "\t Train memory/current_active_mem: 1.4262\n",
      "\t Train memory/current_inactive_mem: 6.7737\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 289.3364\n",
      "\t Train metrics/train/MSE: 290.1153\n",
      "\t Train metrics/train/MVC: 288.6646\n",
      "\t Train time/train: 0.0543\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0543\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0058\n",
      "[batch=16/20]:\n",
      "\t Train time/batch: 15\n",
      "\t Train time/sample: 3840\n",
      "\t Train time/batch_in_epoch: 15\n",
      "\t Train time/sample_in_epoch: 3840\n",
      "\t Train memory/current_allocated_mem: 1.4269\n",
      "\t Train memory/current_active_mem: 1.4269\n",
      "\t Train memory/current_inactive_mem: 5.1665\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 291.1779\n",
      "\t Train metrics/train/MSE: 291.9677\n",
      "\t Train metrics/train/MVC: 290.4140\n",
      "\t Train time/train: 0.0546\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0546\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0044\n",
      "[batch=17/20]:\n",
      "\t Train time/batch: 16\n",
      "\t Train time/sample: 4096\n",
      "\t Train time/batch_in_epoch: 16\n",
      "\t Train time/sample_in_epoch: 4096\n",
      "\t Train memory/current_allocated_mem: 1.4330\n",
      "\t Train memory/current_active_mem: 1.4330\n",
      "\t Train memory/current_inactive_mem: 6.3684\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 297.4088\n",
      "\t Train metrics/train/MSE: 298.3387\n",
      "\t Train metrics/train/MVC: 296.4791\n",
      "\t Train time/train: 0.0548\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0548\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0001\n",
      "\t Train time/remaining_estimate: 0.0031\n",
      "[batch=18/20]:\n",
      "\t Train time/batch: 17\n",
      "\t Train time/sample: 4352\n",
      "\t Train time/batch_in_epoch: 17\n",
      "\t Train time/sample_in_epoch: 4352\n",
      "\t Train memory/current_allocated_mem: 1.4376\n",
      "\t Train memory/current_active_mem: 1.4376\n",
      "\t Train memory/current_inactive_mem: 5.9654\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 292.0077\n",
      "\t Train metrics/train/MSE: 292.7537\n",
      "\t Train metrics/train/MVC: 291.2784\n",
      "\t Train time/train: 0.0550\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0550\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0020\n",
      "[batch=19/20]:\n",
      "\t Train time/batch: 18\n",
      "\t Train time/sample: 4608\n",
      "\t Train time/batch_in_epoch: 18\n",
      "\t Train time/sample_in_epoch: 4608\n",
      "\t Train memory/current_allocated_mem: 1.4300\n",
      "\t Train memory/current_active_mem: 1.4300\n",
      "\t Train memory/current_inactive_mem: 6.2456\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 291.6406\n",
      "\t Train metrics/train/MSE: 292.3736\n",
      "\t Train metrics/train/MVC: 290.9198\n",
      "\t Train time/train: 0.0552\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0552\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0010\n",
      "[batch=20/20]:\n",
      "\t Train time/batch: 19\n",
      "\t Train time/sample: 4864\n",
      "\t Train time/batch_in_epoch: 19\n",
      "\t Train time/sample_in_epoch: 4864\n",
      "\t Train memory/current_allocated_mem: 1.4282\n",
      "\t Train memory/current_active_mem: 1.4282\n",
      "\t Train memory/current_inactive_mem: 6.2474\n",
      "\t Train memory/current_reserved_mem: 27.9780\n",
      "\t Train memory/peak_allocated_mem: 28.0480\n",
      "\t Train memory/peak_active_mem: 28.0480\n",
      "\t Train memory/peak_inactive_mem: 10.1910\n",
      "\t Train memory/peak_reserved_mem: 28.3470\n",
      "\t Train memory/alloc_retries: 1\n",
      "\t Train trainer/device_train_microbatch_size: 128\n",
      "\t Train loss/train/total: 287.2236\n",
      "\t Train metrics/train/MSE: 287.8971\n",
      "\t Train metrics/train/MVC: 286.5370\n",
      "\t Train time/train: 0.0555\n",
      "\t Train time/val: 0.0000\n",
      "\t Train time/total: 0.0555\n",
      "\t Train lr-DecoupledAdamW/group0: 0.0000\n",
      "\t Train time/remaining_estimate: 0.0000\n",
      "[Eval batch=1/100] Eval on eval data\n",
      "[Eval batch=11/100] Eval on eval data\n",
      "[Eval batch=21/100] Eval on eval data\n",
      "[Eval batch=31/100] Eval on eval data\n",
      "[Eval batch=41/100] Eval on eval data\n",
      "[Eval batch=50/100] Eval on eval data\n",
      "[Eval batch=60/100] Eval on eval data\n",
      "[Eval batch=70/100] Eval on eval data\n",
      "[Eval batch=80/100] Eval on eval data\n",
      "[Eval batch=90/100] Eval on eval data\n",
      "[Eval batch=100/100] Eval on eval data:\n",
      "\t Eval metrics/eval/MSE: 266.6998\n",
      "\t Eval metrics/eval/MVC: 266.2087\n",
      "\t Eval metrics/eval/Spearman: 0.1687\n",
      "2025-10-23 21:17:19,605: rank0[427769][MainThread]: INFO: train: Training finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and checkpoints saved at ./checkpoints/custom_test_run!\n"
     ]
    }
   ],
   "source": [
    "# Import train module from scripts directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../scripts'))\n",
    "from train import main\n",
    "\n",
    "cfg = om.load(custom_config_path)\n",
    "\n",
    "# Train the model\n",
    "trainer = main(cfg)\n",
    "\n",
    "print(f\"Training completed and checkpoints saved at {save_folder}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Train using Composer CLI\n",
    "\n",
    "Alternatively, you can train using the command line with composer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training via shell command\n",
    "!composer ../scripts/train.py -f {custom_config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Resume Training\n",
    "\n",
    "Note that if your run stopped unexpectedly and you want to resume the training from where it stopped, simply use the **same `run_name` and `save_folder`** in the configuration. The trainer will automatically pick up from the last saved checkpoint.\n",
    "The trainer will automatically detect existing checkpoints and resume with full state (model weights, optimizer, scheduler, etc.).\n",
    "\n",
    "```python\n",
    "resume_cfg = om.load(custom_config_path)\n",
    "\n",
    "# Keep the same run_name and save_folder - training will auto-resume\n",
    "trainer = main(resume_cfg)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tuning a Pre-trained Model\n",
    "\n",
    "When loading from a checkpoint, you have two options:\n",
    "\n",
    "**Option 1:** Full Recovery\n",
    "- Set `load_path` to your checkpoint directory or file\n",
    "- Loads both model weights AND optimizer/scheduler states\n",
    "\n",
    "```python\n",
    "cfg.load_path = \"s3://bucket/path/to/checkpoint/\"\n",
    "# This recovers everything: weights + optimizer + scheduler\n",
    "```\n",
    "\n",
    "**Option 2:** Weights Only \n",
    "- Set `load_path` AND `load_weights_only=True`\n",
    "- Loads **only model weights**, optimizer/scheduler are initialized fresh\n",
    "\n",
    "```python\n",
    "cfg.load_path = \"s3://bucket/path/to/checkpoint/\"\n",
    "cfg.load_weights_only = True\n",
    "# This loads only weights, optimizer/scheduler start fresh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning configuration:\n",
      "seed: 777\n",
      "device_train_batch_size: 100\n",
      "global_train_batch_size: 100\n",
      "device_eval_batch_size: 100\n",
      "device_train_microbatch_size: auto\n",
      "vocabulary:\n",
      "  remote: s3://tahoe-hackathon-data/MFM/vevo_v2_vocab.json\n",
      "  local: vocab.json\n",
      "model:\n",
      "  name: tahoe_x1\n",
      "  d_model: 512\n",
      "  n_layers: 12\n",
      "  init_device: cpu\n",
      "  expansion_ratio: 4\n",
      "  standard_scale_outputs: false\n",
      "  transformer_activation: relu\n",
      "  n_heads: 8\n",
      "  norm_scheme: pre\n",
      "  use_generative_training: true\n",
      "  use_cell_conditioned_generation: false\n",
      "  use_glu: false\n",
      "  cell_emb_style: cls\n",
      "  attn_config:\n",
      "    attn_impl: flash\n",
      "    attn_type: grouped_query_attention\n",
      "    kv_nheads: 8\n",
      "    attn_pdrop: 0.0\n",
      "    use_attn_mask: false\n",
      "  norm_config:\n",
      "    norm_type: layernorm\n",
      "    eps: 1.0e-05\n",
      "  expression_encoder:\n",
      "    input_emb_style: continuous\n",
      "    dropout: 0.1\n",
      "    max_value: 512\n",
      "    activation: relu\n",
      "    use_norm: true\n",
      "  gene_encoder:\n",
      "    use_norm: true\n",
      "  mvc:\n",
      "    arch_style: inner product\n",
      "    query_activation: sigmoid\n",
      "    scaled_dot_product: true\n",
      "  expression_decoder:\n",
      "    n_outputs: 1\n",
      "    n_layers: 1\n",
      "    activation: leaky_relu\n",
      "collator:\n",
      "  do_padding: true\n",
      "  pad_value: -2\n",
      "  do_mlm: true\n",
      "  do_binning: true\n",
      "  mlm_probability: 0.5\n",
      "  mask_value: -1\n",
      "  max_length: 1024\n",
      "  sampling: true\n",
      "  data_style: both\n",
      "  num_bins: 51\n",
      "  right_binning: false\n",
      "  use_junk_tokens: false\n",
      "train_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/train/\n",
      "        local: mds-data-folder/cellxgene/train\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: true\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "valid_loader:\n",
      "  dataset:\n",
      "    streams:\n",
      "      cellxgene:\n",
      "        remote: s3://tahoe-hackathon-data/MFM/cellxgene_2025_01_21_merged_MDS/valid/\n",
      "        local: mds-data-folder/cellxgene/val\n",
      "    download_timeout: 300\n",
      "    allow_unsafe_types: true\n",
      "    shuffle: false\n",
      "    shuffle_seed: 777\n",
      "    num_canonical_nodes: 2\n",
      "  drop_last: false\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  prefetch_factor: 48\n",
      "  persistent_workers: true\n",
      "optimizer:\n",
      "  name: decoupled_adamw\n",
      "  lr: 1.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.95\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 1.0e-06\n",
      "scheduler:\n",
      "  name: constant_with_warmup\n",
      "  t_warmup: 0ba\n",
      "algorithms:\n",
      "  gradient_clipping:\n",
      "    clipping_type: norm\n",
      "    clipping_threshold: 1.0\n",
      "  low_precision_layernorm: {}\n",
      "precision: amp_bf16\n",
      "eval_interval: 1000ba\n",
      "eval_subset_num_batches: 100\n",
      "max_duration: 30ba\n",
      "fsdp_config:\n",
      "  sharding_strategy: FULL_SHARD\n",
      "  mixed_precision: DEFAULT\n",
      "  activation_checkpointing: false\n",
      "  activation_checkpointing_reentrant: false\n",
      "  activation_cpu_offload: false\n",
      "  limit_all_gathers: true\n",
      "  verbose: true\n",
      "callbacks:\n",
      "  speed_monitor:\n",
      "    window_size: 20\n",
      "  lr_monitor: {}\n",
      "  memory_monitor: {}\n",
      "  runtime_estimator: {}\n",
      "loggers:\n",
      "  wandb:\n",
      "    project: tahoe_x1\n",
      "    log_artifacts: false\n",
      "save_folder: ./checkpoints/finetuned_{run_name}\n",
      "save_interval: 2000ba\n",
      "load_path: s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load configuration for fine-tuning\n",
    "finetune_cfg = om.load(\"../configs/test_run.yaml\")\n",
    "\n",
    "# Set checkpoint path\n",
    "checkpoint_path = \"s3://tahoe-hackathon-data/MFM/ckpts/70m/best-model.pt\"  # Or local path\n",
    "finetune_cfg.load_path = checkpoint_path\n",
    "\n",
    "# Adjust learning rate for fine-tuning and schedular for finetuning\n",
    "finetune_cfg.optimizer.lr = 1.0e-5\n",
    "finetune_cfg.optimizer.weight_decay = 1.0e-6\n",
    "finetune_cfg.scheduler = {}\n",
    "finetune_cfg.scheduler.name = 'constant_with_warmup'\n",
    "finetune_cfg.scheduler.t_warmup = '0ba'\n",
    "\n",
    "finetune_cfg.max_duration = \"30ba\"\n",
    "\n",
    "# Update save folder\n",
    "finetune_cfg.save_folder = \"./checkpoints/finetuned_{run_name}\"\n",
    "\n",
    "print(\"Fine-tuning configuration:\")\n",
    "print(om.to_yaml(finetune_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "# Note: main was imported in cell 5, but if running this cell standalone, ensure it's imported:\n",
    "# import sys, os\n",
    "# sys.path.insert(0, os.path.abspath('../scripts'))\n",
    "# from train import main\n",
    "\n",
    "finetune_trainer = main(finetune_cfg)\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "\n",
    "1. You can monitor your training in **Weights & Biases**\n",
    "2. Model checkpoints are saved according to `save_interval`\n",
    "3. If you encounter OOM issues try reducing the `device_train_batch_size`\n",
    "4. If you are using a single GPU you can remove the `fsdp_config` from your custom configuration.\n",
    "5. Ensure `attn_impl: flash` and `use_attn_mask: False` as Triton backend is no longer supported by our codebase (Email us if you have questions on how to use triton backend with custom attn masking)\n",
    "6. You can add `cell_classification` and `marginal_essentiality` callbacks to the configuration files so that the model will authomathically be evaluated on these benchmarks. (adding some samples is TODO)\n",
    "\n",
    "7. After training you can:\n",
    "    1. **Prepare model for inference**: Use `scripts/inference/prepare_for_inference.py`\n",
    "    2. **Extract cell and gene embeddings**: See `clustering_tutorial.ipynb` and `inference.predict_embeddings`\n",
    "    3. **Run benchmarks**: See `scripts/depmap/` and `scripts/msigdb/`\n",
    "    4. **Upload to HuggingFace**: For sharing your trained model\n",
    "\n",
    "For more details, refer to the [README.md](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahoe-x1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
