integrations:
- integration_type: git_repo
  git_repo: vevotx/vevo-scGPT
  git_branch: dev-temp
  # git_commit:  # OR use your commit hash
  pip_install: -e . --no-deps
  ssh_clone: true  # Should be true if using a private repo

# We are fetching, converting, and training on the 'val' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `llm-foundry/scripts/train/README.md`
# to convert and host the full 'train' dataset.
command: |
  cd vevo-scGPT/scripts
  composer train.py /mnt/config/parameters.yaml
image: vevotx/ml-scgpt:shreshth
env_variables:
  MOSAICML_PLATFORM: False #Logging metadata to MosaicML platform is disabled, since it seems to timeout
name: scgpt-debug-70m

compute:
  gpus: 16 # Number of GPUs to use
  cluster: r14z3
scheduling:
  max_duration: 0.25 # Maximum duration of the run in hours

  ## These configurations are optional
  # cluster: # TODO # Name of the cluster to use for this run
  # gpu_type: a100_80gb # Type of GPU to use. We use a100_80gb in our experiments


# The below is injected as a YAML file: /mnt/config/parameters.yaml
# but is not used in this example.
parameters:
  global_seed: 777
  seed: ${global_seed}
  device_train_batch_size: 128
  global_train_batch_size: 2048
  device_eval_batch_size: 128
  device_train_microbatch_size: "auto"
  model:
    name: vevo_scgpt
    d_model: 512
    n_layers: 12
    init_device: meta
    expansion_ratio: 4.0
    transformer_activation: relu
    n_heads: 8
    norm_scheme: "pre"
    use_generative_training: True
    use_cell_conditioned_generation: False
    use_glu: False
    cell_emb_style: cls
    attn_config:
      attn_impl: triton
      attn_type: "grouped_query_attention"
      kv_nheads: 8
      attn_pdrop: 0.0
    norm_config:
      norm_type: "layernorm"
      eps: 1.0e-5
    expression_encoder:
      input_emb_style: "continuous"
      dropout: 0.1
      max_value: 512
      activation: relu
      use_norm: True
    gene_encoder:
      use_norm: True
    mvc:
      arch_style: "inner product"
      query_activation: "sigmoid"
    expression_decoder:
      n_outputs: 1
      n_layers: 2
      activation: "leaky_relu"
  vocabulary:
    remote: "s3://vevo-ml-datasets/vevo-scgpt/datasets/cellxgene_primary_2023-12-15_MDS/cellxgene_primary_2023-12-15_vocab.json"
    local: "mds-data-folder/vocab.json"
  collator:
    do_padding: True
    pad_value: -2
    do_mlm: True
    do_binning: True
    mlm_probability:
     - 0.25
     - 0.50
     - 0.75
    mask_value: -1
    max_length: 1200
    sampling: True
    data_style: "both"
    num_bins: 51
    right_binning: False
    use_junk_tokens: True
  train_loader:
    dataset:
      remote: "s3://vevo-ml-datasets/vevo-scgpt/datasets/cellxgene_primary_2023-12-15_MDS_v2/train"
      local: "mds-data-folder/train"
      download_timeout: 300
      allow_unsafe_types: True
      shuffle: True
      shuffle_seed: ${global_seed}
      num_canonical_nodes: 2
    drop_last: False
    num_workers: 8
    pin_memory: True
    prefetch_factor: 48
    persistent_workers: True
  valid_loader:
    dataset:
      remote: "s3://vevo-ml-datasets/vevo-scgpt/datasets/cellxgene_primary_2023-12-15_MDS_v2/val"
      local: "mds-data-folder/val"
      download_timeout: 300
      allow_unsafe_types: True
      shuffle: False
      shuffle_seed: ${global_seed}
      num_canonical_nodes: 2
    drop_last: False
    num_workers: 8
    pin_memory: True
    prefetch_factor: 48
    persistent_workers: True
  optimizer:
    name: decoupled_adamw
    lr: 2.0e-4
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 1.0e-05
  scheduler:
    name: cosine_with_warmup
    t_warmup: "0.05dur"
    t_max: "1dur"
    alpha_f: 0.1
  algorithms:
    gradient_clipping:
      clipping_type: norm
      clipping_threshold: 1.0
  precision: amp_bf16
  eval_interval: "2000ba"
  max_duration: "12ep"
  fsdp_config:
    sharding_strategy: FULL_SHARD
    mixed_precision: DEFAULT
    activation_checkpointing: false
    activation_checkpointing_reentrant: false
    activation_cpu_offload: false
    limit_all_gathers: true
    verbose: true
  callbacks:
    speed_monitor:
      window_size: 20
    lr_monitor: {}
    memory_monitor: {}
    runtime_estimator: {}
  loggers:
    wandb:
      project: vevo-scgpt
      log_artifacts: False
  save_folder: "s3://vevo-ml-datasets/vevo-scgpt/models/{run_name}"
  save_interval: "2000ba"
  autoresume: True
  run_name: "scgpt-70m-1200"